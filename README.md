# ğŸ›« Flight-Pipeline

_Pipeline Spark (Scala) pour la prÃ©paration et la jointure temporelle des donnÃ©es vols & mÃ©tÃ©o_

---

## ğŸ“˜ Objectif

Ce projet met en Å“uvre un pipeline **Apache Spark / Scala** capable de :
- lire et nettoyer les donnÃ©es de vols (Flights) et de mÃ©tÃ©o (Weather),
- enrichir les vols avec les observations mÃ©tÃ©o correspondantes (origine et destination),
- produire des tables Delta (flight_clean, weather_clean, airport_timezone_clean),
- gÃ©nÃ©rer des jeux joints exploitables pour la modÃ©lisation (ex. prÃ©diction de retards).

Le pipeline fonctionne :
- **en local (WSL/Linux)** pour dÃ©veloppement et tests,
- **sur le cluster Hadoop/YARN du LAMSADE** pour exÃ©cution distribuÃ©e.

---

## âš™ï¸ Environnement requis

### Local (WSL/Linux)
- **Java 11**
- **Scala 2.12.x**
- **sbt â‰¥ 1.10.7**
- **Spark 3.5.x**
- AccÃ¨s rÃ©seau au dÃ©pÃ´t Maven central
- (Optionnel) Delta Lake 3.2.0 pour Spark local

### Cluster LAMSADE
- AccÃ¨s SSH via bastion ssh.lamsade.dauphine.fr
- Espace HDFS autorisÃ© : /students/p6emiasd2025/nvrel
- Spark 3.5.1 et YARN disponibles sur vmhadoopmaster.srv.lamsade.dauphine.fr

---

## ğŸ—ï¸ Structure du projet
```
flight-pipeline/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/scala/flightpipeline/
â”‚   â”‚   â”œâ”€â”€ Main.scala
â”‚   â”‚   â”œâ”€â”€ stage/...
â”‚   â”‚   â””â”€â”€ util/...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ submit-local.sh
â”‚   â”œâ”€â”€ submit-cluster.sh
â”‚   â””â”€â”€ env-local.sh
â”œâ”€â”€ run_cluster_sample.sh     â† script d'exÃ©cution cluster
â”œâ”€â”€ data/                      â† donnÃ©es locales (non versionnÃ©es)
â”œâ”€â”€ out/                       â† sorties locales (non versionnÃ©es)
â””â”€â”€ README.md
```

---

## ğŸ§° Installation locale et build du JAR

Depuis la racine du projet :
```bash
sbt clean assembly
```

Le JAR exÃ©cutable est gÃ©nÃ©rÃ© ici : `target/scala-2.12/flight-pipeline-assembly-0.1.0.jar`

---

## ğŸ§ª ExÃ©cution locale (WSL)
```bash
scripts/submit-local.sh prepare
scripts/submit-local.sh join 0
```

Les sorties apparaissent dans `out/` :
```
out/flight_clean.parquet
out/weather_clean.parquet
out/join_intermediate.parquet
```

---

## â˜ï¸ DÃ©ploiement sur le cluster LAMSADE

### 1ï¸âƒ£ AccÃ¨s SSH (bastion + edge node)

Ajouter ceci Ã  `~/.ssh/config` :
```
Host lamsade
    HostName ssh.lamsade.dauphine.fr
    Port 5022
    User nvrel
    IdentityFile ~/.ssh/id_ed25519_nvrel
    IdentitiesOnly yes
    IdentityAgent none

Host vmhadoopmaster
    HostName vmhadoopmaster.srv.lamsade.dauphine.fr
    User nvrel
    IdentityFile ~/.ssh/id_ed25519_nvrel
    IdentitiesOnly yes
    ProxyJump lamsade
```

VÃ©rifier :
```bash
ssh -J lamsade vmhadoopmaster 'hostname && whoami'
```

### 2ï¸âƒ£ Transfert du JAR et des donnÃ©es (SFTP via bastion)

Depuis la machine locale :
```bash
sftp -P 5022 -i ~/.ssh/id_ed25519_nvrel -o IdentitiesOnly=yes -o IdentityAgent=none \
  nvrel@ssh.lamsade.dauphine.fr <<'SFTP'
mkdir -p workspace/apps
mkdir -p workspace/data
cd workspace
put target/scala-2.12/flight-pipeline-assembly-0.1.0.jar apps/
put data/Flights/201201.csv data/
put data/Weather/201201hourly.txt data/
put data/wban_airport_timezone.csv data/
bye
SFTP
```

### 3ï¸âƒ£ Connexion au cluster
```bash
ssh -J lamsade vmhadoopmaster
```

### 4ï¸âƒ£ PrÃ©parer HDFS dans ton espace autorisÃ©
```bash
export HDFS_BASE="/students/p6emiasd2025/nvrel"
export HDFS_DATA="$HDFS_BASE/flight-pipeline/data"
export HDFS_OUT="$HDFS_BASE/flight-pipeline/out-sample"
export HDFS_STAGING="$HDFS_BASE/.sparkStaging"

hdfs dfs -mkdir -p "$HDFS_DATA/Flights" "$HDFS_DATA/Weather" "$HDFS_OUT" "$HDFS_STAGING"
hdfs dfs -chmod 700 "$HDFS_STAGING"

hdfs dfs -put -f ~/workspace/data/201201.csv "$HDFS_DATA/Flights/"
hdfs dfs -put -f ~/workspace/data/201201hourly.txt "$HDFS_DATA/Weather/"
hdfs dfs -put -f ~/workspace/data/wban_airport_timezone.csv "$HDFS_DATA/"
```

### 5ï¸âƒ£ ExÃ©cution sur YARN

a) RÃ©soudre le chemin du JAR
```bash
APP_JAR=$(readlink -f ~/workspace/apps/flight-pipeline-assembly-0.1.0.jar)
```

b) Lancer `prepare` (Ã©chantillon janvier 2012)
```bash
spark-submit \
  --class flightpipeline.Main \
  --master yarn \
  --deploy-mode cluster \
  --packages io.delta:delta-spark_2.12:3.2.0 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  --conf spark.yarn.stagingDir="hdfs://$HDFS_STAGING" \
  --driver-memory 4g \
  --executor-memory 6g \
  --executor-cores 3 \
  --num-executors 6 \
  "$APP_JAR" \
  --mode=prepare \
  --flights="hdfs://$HDFS_DATA/Flights" \
  --weather="hdfs://$HDFS_DATA/Weather" \
  --airport="hdfs://$HDFS_DATA/wban_airport_timezone.csv" \
  --out="hdfs://$HDFS_OUT" \
  --hours=12 --lags=0 --sample-month=201201
```

c) Lancer `join`
```bash
spark-submit \
  --class flightpipeline.Main \
  --master yarn \
  --deploy-mode cluster \
  --packages io.delta:delta-spark_2.12:3.2.0 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  --conf spark.yarn.stagingDir="hdfs://$HDFS_STAGING" \
  --driver-memory 4g \
  --executor-memory 6g \
  --executor-cores 3 \
  --num-executors 6 \
  "$APP_JAR" \
  --mode=join \
  --flights="hdfs://$HDFS_DATA/Flights" \
  --weather="hdfs://$HDFS_DATA/Weather" \
  --airport="hdfs://$HDFS_DATA/wban_airport_timezone.csv" \
  --out="hdfs://$HDFS_OUT" \
  --hours=12 --lags=0
```

### 6ï¸âƒ£ VÃ©rification des sorties
```bash
hdfs dfs -ls "$HDFS_OUT/flight_clean.parquet/_delta_log"
hdfs dfs -ls "$HDFS_OUT/weather_clean.parquet/_delta_log"
hdfs dfs -ls "$HDFS_OUT/join_intermediate.parquet/_delta_log"
```

Sur ton poste, tu peux accÃ©der Ã  l'interface YARN :
```bash
ssh -f -N -L 18088:127.0.0.1:8088 vmhadoopmaster
```

Puis ouvrir ğŸ‘‰ http://localhost:18088

### 7ï¸âƒ£ Script automatisÃ©

Le script `run_cluster_sample.sh` (dÃ©jÃ  inclus dans le projet) permet d'automatiser les Ã©tapes `prepare` et `join` :
```bash
# Syntaxe : ./run_cluster_sample.sh [YYYYMM] [LAGS]
./run_cluster_sample.sh 201201 0
```

Il gÃ¨re :
- la crÃ©ation des dossiers HDFS,
- l'upload des fichiers,
- la configuration de staging YARN,
- les soumissions Spark (prepare puis join),
- et les vÃ©rifications finales.

---

## ğŸ“Š RÃ©sultats attendus

Sur HDFS (`/students/p6emiasd2025/nvrel/flight-pipeline/out-sample/`) :
```
â”œâ”€â”€ flight_clean.parquet/
â”‚   â””â”€â”€ _delta_log/
â”œâ”€â”€ weather_clean.parquet/
â”‚   â””â”€â”€ _delta_log/
â”œâ”€â”€ airport_timezone_clean.parquet/
â”‚   â””â”€â”€ _delta_log/
â”œâ”€â”€ join_intermediate.parquet/
â”‚   â””â”€â”€ _delta_log/
â””â”€â”€ quality/
```

---

## ğŸ’¡ Bonnes pratiques

- Toujours travailler dans l'espace HDFS personnel (`/students/p6emiasd2025/nvrel`).
- Ne jamais Ã©crire sous `/user` ni `/tmp` du cluster.
- Ã‰viter les `~` ou espaces dans les chemins Spark : utilise toujours des chemins absolus.
- Nettoyer les anciennes sorties avant un nouveau run :
```bash
  hdfs dfs -rm -r -f /students/p6emiasd2025/nvrel/flight-pipeline/out-sample
```
- Pour tester plusieurs mois :
```bash
  ./run_cluster_sample.sh 201202 0
```

---

## ğŸ“‹ Licence

MIT License Â© 2025 â€” Nicolas Vrel